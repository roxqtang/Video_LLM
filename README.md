# Video_LLM

## Related Papers

| Category | Paper | Authors | Published Date | Conference/Journal |
|----------|-------|---------|---------------|-------------------|
| **Video Understanding** | [VideoLLM: Modeling Video Understanding with Large Language Models](https://arxiv.org/abs/2305.13292) | Guo, et al. | May 2023 | arXiv |
| | [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858) | Zhang, et al. | June 2023 | ICCV 2023 |
| | [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) | Dai, et al. | Nov 2023 | arXiv |
| **Video Generation** | [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://arxiv.org/abs/2312.14125) | Yu, et al. | Dec 2023 | arXiv |
| | [Gen-2: The Next Step Forward in General-Purpose Video Models](https://arxiv.org/abs/2312.05612) | Esser, et al. | Dec 2023 | arXiv |
| | [VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models](https://arxiv.org/abs/2502.02492) | Chefer, et al. | Feb 2025 | arXiv |
| | [Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis](https://arxiv.org/abs/2401.10797) | Misra, et al. | Jan 2024 | arXiv |
| **Vision-Language Models** | [LLaVA1 : Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | Liu, et al. | Dec 2023 | arXiv |
| | [LLaVA-1.5: Visual Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) | Liu, et al. | Dec 2023 | arXiv |
| | [Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers](https://arxiv.org/abs/2401.05159) | Darrell, et al. | Jan 2024 | arXiv |
| | [Analyzing The Language of Visual Tokens](https://arxiv.org/abs/2402.18476) | Misra, Darrell, et al. | Feb 2024 | arXiv |
| | [CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning](https://arxiv.org/abs/2402.09575) | Du, et al. | Feb 2024 | arXiv |
| | [CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts](https://arxiv.org/abs/2401.13601) | Shi, et al. | Jan 2024 | arXiv |
| **Vision Transformers** | [Token Merging: Your ViT But Faster](https://openreview.net/pdf?id=JroZRaRw7Eu) | Bolya, et al. | Mar 2023 | ICLR 2023 |
| | [Scaling White-Box Transformers for Vision](https://arxiv.org/abs/2402.06039) | Ma, et al. | Feb 2024 | arXiv |
| | [White-Box Transformers via Sparse Rate Reduction](https://proceedings.neurips.cc/paper_files/paper/2023/hash/65d2ea03425887a717c435081cfc5dbb-Abstract-Conference.html) | Ma, et al. | Dec 2023 | NeurIPS 2023 |
| | [Eyes wide shut](https://arxiv.org/abs/2401.14167) | Ma, et al. | Jan 2024 | arXiv |
| **Language Models** | [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) | Beltagy, et al. | Apr 2020 | arXiv |
| | [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | Radford, et al. | 2019 | OpenAI |
| | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) | Touvron, et al. | Feb 2023 | arXiv |
| | [DeepSeekV3: Technical Report](https://arxiv.org/html/2412.19437v1) | Deeoseek-AI | Dec 2024 | arXiv |
| | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) | Guo, et al. | Jan 2025 | arXiv |
| **Model Compression** | [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149) | Han, et al. | Oct 2015 | ICLR 2016 |
| | [Learning Structured Sparsity in Deep Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf) | Wen, et al. | 2016 | NeurIPS 2016 |
| | [Rethinking the Value of Network Pruning](https://openreview.net/pdf?id=rJlnB3C5Ym) | Liu, et al. | 2019 | ICLR 2019 |
| | [A Simple and Effective Pruning Approach for Large Language Models](https://openreview.net/pdf?id=PxoFut3dWW) | Frantar, et al. | 2023 | ICLR 2023 |
| | [SVDQuant](https://arxiv.org/abs/2402.05917) | Zhu, et al. | Feb 2024 | arXiv |
| **Quantization** | [Trained Ternary Quantization](https://openreview.net/pdf?id=S1_pAu9xl) | Zhu, et al. | 2017 | ICLR 2017 |
| | [Incremental Network Quantization](https://openreview.net/pdf?id=HyQJ-mclg) | Zhou, et al. | 2017 | ICLR 2017 |
| | [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://ieeexplore.ieee.org/abstract/document/8578384) | Jacob, et al. | 2018 | CVPR 2018 |
| | [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://proceedings.mlr.press/v202/xiao23c.html) | Xiao, et al. | 2023 | ICML 2023 |
| **Neural Architecture & Optimization** | [Learning from Multiple Teacher Networks](https://dl.acm.org/doi/10.1145/3097983.3098135) | You, et al. | 2017 | KDD 2017 |
| | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://openreview.net/pdf?id=rJlnB3C5Ym) | Wei, et al. | 2022 | NeurIPS 2022 |
| | [MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://ieeexplore.ieee.org/abstract/document/8954198) | Tan, et al. | 2019 | CVPR 2019 |
| | [FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search](https://ieeexplore.ieee.org/abstract/document/8953587) | Wu, et al. | 2019 | CVPR 2019 |
| **Efficient Training & Inference** | [Neural Gradients Are Near-Lognormal: Improved Quantized and Sparse Training](https://openreview.net/pdf?id=EoFNy62JgDd) | Dong, et al. | 2023 | ICLR 2023 |
| | [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/pdf?id=nZeVKeeFYf9) | Hu, et al. | 2021 | NeurIPS 2021 |
| | [COAT: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training](https://arxiv.org/abs/2410.19313) | Xu, et al. | Oct 2023 | arXiv |
| | [Federated Optimization in Heterogeneous Networks](https://proceedings.mlsys.org/paper_files/paper/2020/hash/1f5fe83998a09396ebe6477d9475ba0c-Abstract.html) | Li, et al. | 2020 | MLSys 2020 |
| **Distributed & Mobile Computing** | [TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning](https://proceedings.neurips.cc/paper/2017/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html) | Wen, et al. | 2017 | NeurIPS 2017 |
| | [Modnn: Local Distributed Mobile Computing System for Deep Neural Networks](https://ieeexplore.ieee.org/document/7927211) | Mao, et al. | 2017 | IEEE VR 2017 |
| | [MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://openreview.net/pdf?id=PEpUb0bfJv) | Chen, et al. | 2024 | ICLR 2024 |
| | [Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting](https://arxiv.org/abs/2404.18911) | Xu, et al. | Apr 2024 | arXiv |
| **Continuous Tokenization** | [Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens](https://arxiv.org/abs/2410.13863) | Fan, et al. | Oct 2024 | arXiv |
| | [Autoregressive Image Generation without Vector Quantization](https://arxiv.org/abs/2406.11838) | Chen, et al. | Jun 2024 | arXiv |
| | [Rank-N-Contrast: Learning Continuous Representations for Regression](https://proceedings.neurips.cc/paper_files/paper/2023/hash/39e9c5913c970e3e49c2df629daff636-Abstract-Conference.html) | Zha, et al. | 2023 | NeurIPS 2023 |
| | [Return of Unconditional Generation: A Self-supervised Representation Generation Method](https://openreview.net/forum?id=clTa4JFBML) | Li, et al. | Sep 2024 | NeurIPS 2024 oral |
| **Multimodal** | [H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation](https://arxiv.org/abs/2307.01486) | Shi, et al. | Jul 2023 | MICCAI 2023 |
| | [CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion](https://arxiv.org/abs/2211.14461) | Zhao, et al. | Apr 2023 | CVPR 2023 |

# Qwen2.5-VL 模型实现与预训练权重加载

本项目是Qwen2.5-VL模型的自定义实现，并提供了加载官方预训练权重的功能。Qwen2.5-VL是通义千问团队开发的多模态大语言模型，结合了强大的视觉和语言理解能力。

## 项目结构

```
├── myqwen.py                  # 模型架构实现
├── modules/
│   ├── vision_encoder.py      # 视觉编码器实现
│   ├── m_rope.py              # 多模态旋转位置编码
│   └── video_porocessor.py    # 视频处理器实现
├── load_pretrained.py         # 预训练权重加载工具
├── use_pretrained_model.py    # 使用加载了权重的模型示例
├── Qwen25_VL_Architecture_README.md  # 架构详细说明
└── README.md                  # 本文档
```

## 安装依赖

本项目需要以下依赖:

```bash
pip install torch torchvision safetensors transformers pillow numpy
```

## 加载预训练权重

我们提供了专门的工具来加载huggingface的safetensor格式的预训练权重到自定义实现的模型中。

### 步骤1: 确保权重文件就绪

权重文件应该在以下路径:
```
models/models--remyxai--SpaceQwen2.5-VL-3B-Instruct/snapshots/ff71e2f363d635f5dbbd655bf11ac7f976f94c7d/
```

包括:
- model-00001-of-00002.safetensors
- model-00002-of-00002.safetensors
- model.safetensors.index.json
- config.json
- tokenizer相关文件

### 步骤2: 运行权重加载脚本

```bash
python load_pretrained.py
```

这将:
1. 从配置文件初始化模型
2. 加载预训练权重并映射到自定义模型
3. 保存加载后的模型到`pretrained_myqwen.pth`

### 步骤3: 使用加载了权重的模型

```bash
python use_pretrained_model.py
```

这将展示如何:
1. 加载带有预训练权重的模型
2. 初始化tokenizer
3. 处理图像输入
4. 生成图像描述

## 权重加载原理

我们的`WeightLoader`类处理权重加载过程:

1. **权重映射**: 建立Hugging Face的权重名称到我们自定义模型权重名称的映射
   ```python
   # 映射规则示例
   mapping_rules = [
       ("visual.patch_embed", "visual_encoder.patch_embed"),
       ("model.layers", "llm_blocks"),
       # 更多映射...
   ]
   ```

2. **加载safetensor文件**: 使用safetensors库加载权重文件
   ```python
   weights = load_file(weight_file, device="cpu")
   ```

3. **应用权重**: 将加载的权重应用到模型参数
   ```python
   param.data.copy_(weight_value)
   ```

## 使用预训练模型

通过`use_pretrained_model.py`中的`generate_for_image`函数，您可以轻松使用模型处理图像:

```python
generated_text = generate_for_image(
    model=model,
    tokenizer=tokenizer,
    image=image_tensor,
    prompt="描述这张图片中的内容。",
    device="cuda"  # 或 "cpu"
)
```

## 模型架构说明

Qwen2.5-VL模型由几个关键组件组成:

1. **视觉编码器**: 基于ViT的结构，处理图像输入
2. **多模态旋转位置编码**: 创新的编码机制，同时处理时间和空间信息
3. **视频处理器**: 处理视频帧之间的时间关系
4. **语言模型**: 核心Transformer模块，处理多模态信息并生成文本

更详细的架构说明请参见`Qwen25_VL_Architecture_README.md`。

## 故障排除

### 常见问题

1. **权重加载错误**:
   - 检查safetensor文件是否存在且完整
   - 检查模型结构与权重是否匹配

2. **形状不匹配错误**:
   - 检查模型配置是否与预训练模型一致
   - 查看具体的形状不匹配信息，调整相应的层

3. **内存不足**:
   - 对于GPU内存有限的情况，尝试使用CPU加载(`device="cpu"`)
   - 或使用较小的模型版本

## 后续工作

1. 增强权重映射功能，支持更灵活的映射关系
2. 添加量化支持，减少内存占用
3. 优化对长视频的处理效率

## 致谢

- 感谢通义千问团队开发的优秀多模态模型
- 感谢Hugging Face提供模型托管和分发服务

