# Video_LLM

## Related Papers

### Video Understanding

| Paper | Authors | Published Date | Conference/Journal |
|-------|---------|---------------|-------------------|
| [VideoLLM: Modeling Video Understanding with Large Language Models](https://arxiv.org/abs/2305.13292) | Guo, et al. | May 2023 | arXiv |
| [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858) | Zhang, et al. | June 2023 | ICCV 2023 |
| [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) | Dai, et al. | Nov 2023 | arXiv |

### Video Generation

| Paper | Authors | Published Date | Conference/Journal |
|-------|---------|---------------|-------------------|
| [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://arxiv.org/abs/2312.14125) | Yu, et al. | Dec 2023 | arXiv |
| [Gen-2: The Next Step Forward in General-Purpose Video Models](https://arxiv.org/abs/2312.05612) | Esser, et al. | Dec 2023 | arXiv |

### Efficient Transformer

| Paper | Authors | Published Date | Conference/Journal |
|-------|---------|---------------|-------------------|
| [Token Merging: Your ViT But Faster](https://openreview.net/pdf?id=JroZRaRw7Eu)|  Bolya, et al. | Mar 2023 | ICLR 2023 |
| [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149) | Han, et al. | Oct 2015 | ICLR 2016 |

## Citation

Here is the extracted text from the images:

---

### **Efficient AI Paper List**
#### **[Algorithm] Lecture 03 Feb 05**
1. **Token Merging: Your ViT But Faster**  
   [Link: OpenReview](https://openreview.net/pdf?id=JroZRaRw7Eu)

2. **Longformer: The Long-Document Transformer**  
   [Link: arXiv](https://arxiv.org/abs/2004.05150)

3. **Language Models are Unsupervised Multitask Learners**  
   [Link: OpenAI](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

4. **LLaMA: Open and Efficient Foundation Language Models**  
   [Link: arXiv](https://arxiv.org/abs/2302.13971)

#### **[Algorithm] Lecture 04 Feb 12**
5. **Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding**  
   [Link: arXiv](https://arxiv.org/abs/1510.00149)

6. **Learning Structured Sparsity in Deep Neural Networks**  
   [Link: NeurIPS Proceedings](https://proceedings.neurips.cc/paper_files/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf)

7. **Rethinking the Value of Network Pruning**  
   [Link: OpenReview](https://openreview.net/pdf?id=rJlnB3C5Ym)

8. **A Simple and Effective Pruning Approach for Large Language Models**  
   [Link: OpenReview](https://openreview.net/pdf?id=PxoFut3dWW)

#### **[Algorithm] Lecture 05 Feb 19**
9. **Trained Ternary Quantization**  
   [Link: OpenReview](https://openreview.net/pdf?id=S1_pAu9xl)

10. **Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights**  
    [Link: OpenReview](https://openreview.net/pdf?id=HyQJ-mclg)

11. **Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference**  
    [Link: IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/8578384)

12. **SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models**  
    [Link: ML Research Proceedings](https://proceedings.mlr.press/v202/xiao23c.html)

#### **[Algorithm] Lecture 06 Feb 26**
13. **Learning from Multiple Teacher Networks**  
    [Link: ACM Digital Library](https://dl.acm.org/doi/10.1145/3097983.3098135)

14. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**  
    [Link: OpenReview](https://openreview.net/pdf?id=rJlnB3C5Ym)
Here is the extracted text from the additional image:

---

### **Efficient AI Paper List (Continued)**

#### **[Algorithm] Lecture 06 Feb 26**
15. **MnasNet: Platform-Aware Neural Architecture Search for Mobile**  
    [Link: IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/8954198)

16. **FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search**  
    [Link: IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/8953587)

#### **[Algorithm] Lecture 07 Mar 05**
17. **Neural Gradients Are Near-Lognormal: Improved Quantized and Sparse Training**  
    [Link: OpenReview](https://openreview.net/pdf?id=EoFNy62JgDd)

18. **LoRA: Low-Rank Adaptation of Large Language Models**  
    [Link: OpenReview](https://openreview.net/pdf?id=nZeVKeeFYf9)

19. **COAT: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training**  
    [Link: arXiv](https://arxiv.org/abs/2410.19313)

20. **Federated Optimization in Heterogeneous Networks**  
    [Link: MLSys Proceedings](https://proceedings.mlsys.org/paper_files/paper/2020/hash/1f5fe83998a09396ebe6477d9475ba0c-Abstract.html)

#### **[Algorithm] Lecture 08 Mar 12**
21. **TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning**  
    [Link: NeurIPS Proceedings](https://proceedings.neurips.cc/paper/2017/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html)

22. **Modnn: Local Distributed Mobile Computing System for Deep Neural Networks**  
    [Link: IEEE Xplore](https://ieeexplore.ieee.org/document/7927211)

23. **MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads**  
    [Link: OpenReview](https://openreview.net/pdf?id=PEpUb0bfJv)

24. **Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting**  
    [Link: arXiv](https://arxiv.org/abs/2404.18911)
